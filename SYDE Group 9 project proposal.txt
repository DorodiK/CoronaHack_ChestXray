Topic: Comparison of techniques for feature and region of interest extraction of abnormalities in biomedical image data using machine learning algorithms(Option B)Project Description:The increase in digital imaging methods in the medical field has enabled healthcare providers to generate a large amount of data containing valuable signals and information. An automated and efficient decision-support solution that can assist medical experts in decision making would be desirable as this would alleviate stress, decrease the chance of fatigue prone decision making and reduce patients’ results wait time. This solution should be able to accurately identify, locate, and caption relevant information within a given medical image with minimal or no supervision. The goal of the project is the comparison of several techniques for identification and localization of several regions of interest in biomedical data, with subtasks of1. A holistic interpretation of the biomedical image scene with the goal of deciding whether a given medical image is normal or abnormal.2. Identification and localization of regions of interest or features that correspond to abnormal indications.TechniquesThe approaches we plan to experiment with are listed below:Technique 1:Use the class activation maps of a trained convolutional neural network (CNN) to localize regions of interest in a given biomedical image. The Idea is referenced from the CVPR 2020 paper [2003.13141] Learning a Weakly-Supervised Video Actor-Action Segmentation Model with a Wise Selection. The activation maps contain relevant spatial information about which parts of the image the CNN network is using to make decisions. Based on the assumption that these parts correlate with abnormalities within the biomedical image, we might be able to use them to identify and localize medical abnormalities.Technique 2:Use key point extraction algorithms such as ORB, SIFT, SURF or dimensionality reduction techniques for detection of relevant features within a biomedical image, extract image patches around these detected regions and use these image patches to do either of the following:1. Validate whether detected image regions are abnormal and create a decision boundary using ML models, such as one-class support vector machines.2. Use clustering algorithms to group similar image features, then parameterize (find the parameters that best fit the distribution) each of these clustered feature spaces, then detect abnormalities using the assumption of the least occurrence.Technique 3:Use an Autoencoder network for learning the representation space of the images, and using the reconstruction error as an indication for possible abnormality within a biomedical image. We would train the network on a normal set of data (since we usually have larger normal samples), then experimentally set a threshold for the reconstruction error that would be used to detect abnormal samples.Evaluation Metrics :Use appropriate error metrics as taught in the course, such as AUC or F1 scores.Datasets description:We would test our methods using one or multiple available biomedical image datasets. The links to the datasets are listed below:CoronaHack -Chest X-Ray-Dataset Pulmonary Chest X-Ray Abnormalities Bacteria detection with darkfield microscopy CAT Scan Localization Brain MRI Images for Brain Tumor Detection VinBigData Chest X-ray (Numpy) Chest X Ray Masks and Labels ImageCLEFmed CaptionSimilar works in literature:(Pinho & Costa, 2018) employed six unsupervised learning methods including both traditional and deep learning approaches for concept detection in medical images. Some of the methods used include bag of visual words, denoising and variational autoencoders, Bidirectional GAN and adversarial deep learning approaches.(Lee, Jun, Cho, & Kim) reviewed several deep learning based approaches for medical image data.(Lyndon et al., 2017) presented an end-to-end medical image captioning deep learning network by pairing an image-encoding CNN with a language-generating recurrent neural network (RNN).(Mao et al., 2017) Presented a method for explaining the content of an image with a RNN.Group Names and Number:Group 9: Dorodi Krishty, Osazee Ero, Ludwig Wilhelm WallReferences:1. Pinho, E., & Costa, C. (2018). Unsupervised learning for concept detection in medical images: A comparative analysis. Applied Sciences (Switzerland), 8(8). https://doi.org/10.3390/app80812132. Lee, J., Jun, S., Cho, Y., Lee, H., Kim, G., Seo, J. and Kim, N., 2017. Deep Learning in Medical Imaging: General Overview. Korean Journal of Radiology, 18(4), p.570. doi: 10.3348/kjr.2017.18.4.5703. Lyndon, D., Kumar, A. and Kim, J., 2017. Neural Captioning for the ImageCLEF 2017 Medical Image Challenges. [online] Ceur-ws.org. Available at: <http://ceur-ws.org/Vol-1866/paper_145.pdf> [Accessed 25 February 2021].4. Mao J, Xu W, Yang Y, Wang J, Huang Z, Yuille A. Explain images with multimodal recurrent neural networks. ArXiv.org Web site. [Accessed April 1, 2017]. https://arxiv.org/abs/1410.1090.